{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import  classification_report, accuracy_score\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the paths to the correct directories\n",
    "\n",
    "RESULTS_PATH = './results/prompting/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.txt'\n",
    "CLASSES_PATH = './imagenet1000_clsidx_to_labels.txt'\n",
    "DATA_PATH = './encodings/CLIP-ViT32/'\n",
    "CACHE_PATH = './.cache'\n",
    "\n",
    "CLIP_VARIANT = 'ViT-B/32'  # Needs to be the same variant as the one used for encoding the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextIAA:\n",
    "    \"\"\"This class provides all necessary methods for the PromptBased IAA Experiments\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, res_file: str = '') -> None:\n",
    "        self.labels = []\n",
    "\n",
    "        with open(CLASSES_PATH, 'r') as text:\n",
    "            for line in text:\n",
    "                label = line.split(\"'\")[1]\n",
    "                label = label.split(\",\")[0]\n",
    "                self.labels.append(label)\n",
    "\n",
    "        if(res_file != ''):\n",
    "            self.res_path = res_file\n",
    "        else:\n",
    "            self.res_path = RESULTS_PATH\n",
    "\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model, preprocess = clip.load(CLIP_VARIANT, self.device, download_root=CACHE_PATH)\n",
    "\n",
    "        #Load prepared Test_Data\n",
    "        SPLIT = 'test'\n",
    "        with open(DATA_PATH + SPLIT + '_encodings', 'rb') as file:\n",
    "            self.test_encodings = torch.load(file, map_location=self.device)\n",
    "\n",
    "        with open(DATA_PATH + SPLIT + '_ratings', 'rb') as file:\n",
    "            self.test_ratings = torch.load(file, map_location=self.device)\n",
    "\n",
    "        self.test_encodings = torch.cat(self.test_encodings).squeeze().float()\n",
    "        self.test_ratings = torch.cat(self.test_ratings).squeeze()\n",
    "        self.test_mean = self.test_ratings @ torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).to(self.device).double()\n",
    "\n",
    "        # Load prepared Train_Data\n",
    "        SPLIT = 'train'\n",
    "        with open(DATA_PATH + SPLIT + '_encodings', 'rb') as file:\n",
    "            self.train_encodings = torch.load(file, map_location=self.device)\n",
    "\n",
    "        with open(DATA_PATH + SPLIT + '_ratings', 'rb') as file:\n",
    "            self.train_ratings = torch.load(file, map_location=self.device)\n",
    "\n",
    "        \n",
    "        self.train_encodings = torch.cat(self.train_encodings).squeeze().float()\n",
    "        self.train_ratings = torch.cat(self.train_ratings).squeeze()\n",
    "        self.train_mean = self.train_ratings @ torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).to(self.device).double()\n",
    "\n",
    "    def generate_embeddings_binary(self, prompts: list, mode = 'fixed'):\n",
    "        \"\"\"Generate Text Embeddings for binary task\n",
    "        Args:\n",
    "            prompts (list): Prompts to be embedded by CLIP\n",
    "            mode (str, optional): Prompt mode. One of ['fixed', 'content', 'ensembling']. Defaults to 'fixed'.\n",
    "        Returns:\n",
    "            torch.tensor: Embedded Prompts\n",
    "        \"\"\"        \n",
    "\n",
    "        if(mode == 'fixed'):\n",
    "            text_inputs = torch.cat([clip.tokenize(d) for d in prompts]).to(self.device)\n",
    "            \n",
    "\n",
    "        elif(mode == 'content' or mode == 'ensembling'):\n",
    "\n",
    "            goodlist = []\n",
    "            badlist = []\n",
    "            goodPos = prompts[1].find('*')\n",
    "            badPos = prompts[0].find(\"*\")\n",
    "\n",
    "            for label in self.labels:\n",
    "                good = prompts[1][:goodPos] + label + prompts[1][goodPos+1:]\n",
    "                bad = prompts[0][:badPos] + label + prompts[0][badPos+1:]\n",
    "\n",
    "                goodlist.append(good)\n",
    "                badlist.append(bad)\n",
    "\n",
    "\n",
    "            if (mode == 'ensembling'):\n",
    "                bad_inputs = torch.cat([clip.tokenize(d) for d in badlist]).to(self.device)\n",
    "                good_inputs = torch.cat([clip.tokenize(d) for d in goodlist]).to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    good_features = self.model.encode_text(good_inputs)\n",
    "                    good_features = torch.mean(good_features, 0, keepdim=True)\n",
    "                    bad_features = self.model.encode_text(bad_inputs) \n",
    "                    bad_features = torch.mean(bad_features, 0, keepdim=True)\n",
    "                \n",
    "                text_features = torch.cat((bad_features, good_features))\n",
    "                return text_features\n",
    "\n",
    "            else:\n",
    "\n",
    "                textlist = badlist.copy()\n",
    "                textlist.append(goodlist)\n",
    "\n",
    "                text_inputs = torch.cat([clip.tokenize(d) for d in textlist]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.encode_text(text_inputs)\n",
    "\n",
    "        return text_features\n",
    "\n",
    "    def get_split(self, split: str = 'test', task: str = 'binary'):\n",
    "        \"\"\"Returns the encoded Images and Ratings for a Split\n",
    "\n",
    "        Args:\n",
    "            split (str, optional): 'test' or 'train'. Defaults to 'test'.\n",
    "            task (str, optional): Determines the format of the ratings. One of ['binary', 'continuous']. Defaults to 'binary'.\n",
    "\n",
    "        Returns:\n",
    "            [tuple]: image_features, labels\n",
    "        \"\"\"        \n",
    "\n",
    "        if (split == 'test'):\n",
    "            image_features = self.test_encodings.unsqueeze(0)\n",
    "            mean = self.test_mean \n",
    "        elif (split == 'train'):\n",
    "            image_features = self.train_encodings.unsqueeze(0)\n",
    "            mean = self.train_mean\n",
    "        else:\n",
    "            raise ValueError(\"Only 'test' or 'train' is a valid value for the argument split\")\n",
    "\n",
    "        if (task == 'binary'):\n",
    "            labels = mean > 5.0\n",
    "\n",
    "        elif (task == 'continuous'):\n",
    "            labels = mean    \n",
    "            \n",
    "        return image_features, labels  \n",
    "\n",
    "    def binary_fixed(self, prompt_bad: str, prompt_good: str, split = 'test', verbose = True):\n",
    "        \"\"\"Performs a evaluation using the provided prompts on the DATASET\n",
    "\n",
    "        Args:\n",
    "            prompt_bad (str): Prompt used for predicting unaesthetic images\n",
    "            prompt_good (str): Prompt used for predicting aesthetic images\n",
    "            split (str, optional): Split for the evaluation. Either 'test' or 'train'. Defaults to 'test'.\n",
    "            verbose (bool, optional): Prints a  full classification report. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            float: prediction accuracy    \n",
    "        \"\"\"        \n",
    "\n",
    "        \n",
    "        image_features, labels  = self.get_split(split=split) \n",
    "\n",
    "        text_features = self.generate_embeddings_binary([prompt_bad, prompt_good])\n",
    "        \n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (100.0 * image_features @ text_features.float().T).softmax(dim=-1)        \n",
    "        values, pred = similarity.topk(1)\n",
    "        pred = pred.ravel()\n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"------------------ \\n Binary Task Results + Fixed (Split: {split}, Prompts: {[prompt_bad, prompt_good]}) :\")\n",
    "            print(classification_report(labels.cpu().numpy(), pred.cpu().numpy(), digits=4, target_names= ['bad', 'good']))\n",
    "            \n",
    "            \n",
    "        return(accuracy_score(labels.cpu().numpy(), pred.cpu().numpy()))\n",
    "\n",
    "    def binary_content(self, prompt_bad: str, prompt_good: str, split = 'test', verbose = True):\n",
    "        \"\"\"Performs a evaluation using content-aware-prompts on the binary task.\n",
    "        The Prompts must contain one '*', which marks the place where the content should be inserted.\n",
    "\n",
    "        Args:\n",
    "            prompt_bad (str): Prompt used for predicting unaesthetic images.\n",
    "            prompt_good (str): Prompt used for predicting aesthetic images.\n",
    "            split (str, optional): Split for the evaluation. Either 'test' or 'train'. Defaults to 'test'.\n",
    "            verbose (bool, optional): Prints a  full classification report. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            float: prediction accuracy    \n",
    "        \"\"\"        \n",
    "\n",
    "        image_features, labels  = self.get_split(split=split)  \n",
    "\n",
    "        text_features = self.generate_embeddings_binary([prompt_bad, prompt_good], mode = 'content')\n",
    "        \n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (100.0 * image_features @ text_features.float().T).softmax(dim=-1)        \n",
    "        values, indices = similarity.topk(1)\n",
    "        pred = torch.div(indices.squeeze(), torch.tensor(len(self.labels)), rounding_mode='floor')\n",
    "\n",
    "        #pred = indices >= len(self.labels)\n",
    "        #pred = pred.ravel()\n",
    "        \n",
    "        if(verbose):\n",
    "            print(f\"------------------ \\n Binary Task Results + Content (Split: {split}, Prompts: {[prompt_bad, prompt_good]}) :\")\n",
    "            print(classification_report(labels.cpu().numpy(), pred.cpu().numpy(), digits=4, target_names= ['bad', 'good']))\n",
    "            \n",
    "        \n",
    "        return(accuracy_score(labels.cpu().numpy(), pred.cpu().numpy()))\n",
    "\n",
    "\n",
    "    def binary_ensembling(self, prompt_bad: str, prompt_good: str, split = 'test', verbose = True):\n",
    "        \"\"\"Performs a evaluation using content-aware-prompts on the binary task.\n",
    "        The Prompts must contain one '*', which marks the place where the content should be inserted.\n",
    "\n",
    "        Args:\n",
    "            prompt_bad (str): Prompt used for predicting unaesthetic images.\n",
    "            prompt_good (str): Prompt used for predicting aesthetic images.\n",
    "            split (str, optional): Split for the evaluation. Either 'test' or 'train'. Defaults to 'test'.\n",
    "            verbose (bool, optional): Print a full classification report. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            float: prediction accuracy\n",
    "        \"\"\" \n",
    "\n",
    "        image_features, labels  = self.get_split(split=split)  \n",
    "\n",
    "        text_features = self.generate_embeddings_binary([prompt_bad, prompt_good], mode = 'ensembling')\n",
    "        \n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (100.0 * image_features.squeeze() @ text_features.squeeze().float().T).softmax(dim=-1)        \n",
    "        values, indices = similarity.topk(1)\n",
    "        pred = indices >= 1\n",
    "        pred = pred.ravel()\n",
    "        \n",
    "        if(verbose):\n",
    "            print(f\"------------------ \\n Binary Task Results + Ensembling (Split: {split}, Prompts: {[prompt_bad, prompt_good]}) :\")\n",
    "            print(classification_report(labels.cpu().numpy(), pred.cpu().numpy(), digits=4, target_names= ['bad', 'good']))\n",
    "            \n",
    "        \n",
    "        return(accuracy_score(labels.cpu().numpy(), pred.cpu().numpy()))\n",
    "\n",
    "    def continuous(self, prompt_bad: str, prompt_good: str, split: str = 'test', mode: str = 'fixed', verbose = True):\n",
    "        \"\"\"Performs a evaluation on the DATASET using the provided prompts for the provided split and mode.\n",
    "        If the selected mode is not 'fixed', the Prompts must contain one '*', which marks the place where the content should be inserted.\n",
    "\n",
    "        Args:\n",
    "            prompt_bad (str): A prompt describing unaesthetic images\n",
    "            prompt_good (str): A prompt describing aesthetic images\n",
    "            split (str, optional): Split for the evaluation. Either 'test' or 'train'. Defaults to 'test'.\n",
    "            mode (str, optional): Selects the mode of the prompts, one of ['fixed', 'content', 'ensembling']. Defaults to 'fixed'.\n",
    "            verbose (bool, optional): Prints the results. Defaults to True.. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            float, float: returns the spearman and pearson correlation coefficient\n",
    "        \"\"\"        \n",
    "\n",
    "        image_features, labels  = self.get_split(split=split, task='continuous') \n",
    "\n",
    "        text_features = self.generate_embeddings_binary([prompt_bad, prompt_good], mode = mode)\n",
    "\n",
    "\n",
    "        weights = []\n",
    "        for i in range(int((len(text_features)/2))):\n",
    "            weights.append(-1)\n",
    "        for i in range(int((len(text_features)/2))):\n",
    "            weights.append(1)\n",
    "\n",
    "        weights = torch.Tensor(weights).to(self.device).float()\n",
    "        similarity = (torch.tensor(100.0).to(self.device) * image_features @ text_features.float().T)\n",
    "        similarity = similarity @ weights\n",
    "        \n",
    "        spearman, p = spearmanr(similarity.squeeze().cpu().numpy(), labels.cpu().numpy())\n",
    "        pearson, p2 = pearsonr(similarity.squeeze().cpu().numpy(), labels.cpu().numpy())\n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"------------------ \\n\\n Continuous Task Results (Split: {split}, Mode: {mode}, Prompts: {[prompt_bad, prompt_good]}) :\\n\")\n",
    "            print(f\"Spearman: {spearman}\\n\") \n",
    "            print(f\"Pearson: {pearson}\\n\") \n",
    "\n",
    "        return spearman, pearson   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ \n",
      " Binary Task Results + Fixed (Split: test, Prompts: ['A atrocious picture', 'A outstanding picture']) :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad     0.5681    0.3182    0.4079      7599\n",
      "        good     0.7567    0.8976    0.8212     17952\n",
      "\n",
      "    accuracy                         0.7253     25551\n",
      "   macro avg     0.6624    0.6079    0.6145     25551\n",
      "weighted avg     0.7006    0.7253    0.6983     25551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7252945090211733"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = TextIAA()\n",
    "\n",
    "eval.binary_fixed(\"A atrocious picture\", \"A outstanding picture\")\n",
    "#eval.binary_content(\"A horrible picture, of a #*\", \"A smashing picture, of a #*\", split='test')\n",
    "#eval.continuous(\"A horrible picture\", \"A outstanding picture\")\n",
    "#eval.continuous(\"A horrible picture, of a #*\", \"A outstanding picture, of a #*\", mode = 'ensembling')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('bigdata-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19bb2817fa2cd9516c33a037289cba52204cae7edae7836523ee704a77970aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
